# TEMPLATE
# <MODEL_NAME_YAML>:
#   model_name: <OPENROUTER_MODEL_NAME>
#   endpoint:
#     api_base: http://openrouter.ai/api/v1
#     api_key: ${OPENROUTER_API_KEY}  # IS LOADED AT RUNTIME
#   parameters:
#     max_tokens: <INT_MAX_NUM_TOKENS>
#     temperature: <FLOAT_BETWEEN_0_AND_1>
#     <MAYBE OTHER PARAMS>: ...

# SIMPLE FLOW (SINGLE MODEL APPROACH)
simple_flow:
  worker_model_1:
    model_name: openrouter/meta-llama/llama-3.1-8b-instruct
    endpoint:
      api_base_url: http://openrouter.ai/api/v1
      api_key: ${OPENROUTER_API_KEY}
    parameters:
      max_tokens: 8192
      temperature: 0.5
  worker_model_2:
    model_name: openrouter/microsoft/phi-4  # openrouter/google/gemma-3-12b-it
    endpoint:
      api_base_url: http://openrouter.ai/api/v1
      api_key: ${OPENROUTER_API_KEY}
    parameters:
      max_tokens: 8192
      temperature: 0.5
  worker_model_3:
    model_name: openrouter/qwen/qwen3-14b
    endpoint:
      api_base_url: http://openrouter.ai/api/v1
      api_key: ${OPENROUTER_API_KEY}
    parameters:
      max_tokens: 8192
      temperature: 0.5
  worker_model_4:
    model_name: openrouter/qwen/qwen3-32b
    endpoint:
      api_base_url: http://openrouter.ai/api/v1
      api-key: ${OPENROUTER_API_KEY}
    parameters:
      max-tokens: 8192
      temperature: 0.5
  worker_model_5:
    model_name: openrouter/nvidia/llama-3.3-nemotron-super-49b-v1.5
    endpoint:
      api_base_url: http://openrouter.ai/api/v1
      api_key: ${OPENROUTER_API_KEY}
    parameters:
      max_tokens: 8192
      temperature: 0.5

# CASCADE MODELS
# CASCADE LEVEL 1 - 2 worker models
# TODO: ADD CASCADE LEVEL 1 MODELS

# CASCADE LEVEL 2 - 3 worker models
# TODO: ADD CASCADE LEVEL 2 MODELS

# CASCADE LEVEL 3 - 4 worker models
# TODO: ADD CASCADE LEVEL 3 MODELS

# CASCADE LEVEL 4 - 5 worker models
# TODO: ADD CASCADE LEVEL 4 MODELS

# CASCADE LEVEL 5 - 6 worker models
# TODO: ADD CASCADE LEVEL 5 MODELS

# JUDGE MODEL
# TODO: ADD JUDGE MODEL
